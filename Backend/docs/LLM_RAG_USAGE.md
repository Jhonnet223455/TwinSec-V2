# GuÃ­a de Uso: Sistema LLM + RAG

## ğŸ“š Tabla de Contenidos
1. [Arquitectura del Sistema](#arquitectura)
2. [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n)
3. [Subir y Indexar Libros](#libros)
4. [Uso de la API](#uso-api)
5. [Ejemplos de Prompts](#ejemplos)
6. [Monitoreo de Costos](#costos)

---

## ğŸ—ï¸ Arquitectura del Sistema {#arquitectura}

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario       â”‚
â”‚   (Frontend)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ POST /api/v1/models/generate
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Backend                                â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM Service                             â”‚  â”‚
â”‚  â”‚                                          â”‚  â”‚
â”‚  â”‚  1. Recibe prompt del usuario           â”‚  â”‚
â”‚  â”‚  2. Consulta RAG para contexto          â”‚  â”‚
â”‚  â”‚  3. Enriquece prompt con teorÃ­a         â”‚  â”‚
â”‚  â”‚  4. Llama a LLM (OpenAI/Anthropic)      â”‚  â”‚
â”‚  â”‚  5. Valida JSON generado                â”‚  â”‚
â”‚  â”‚  6. Guarda en BD (Model + LLMRequest)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RAG Service â”‚â—„â”€â”€â”€â”€â–ºâ”‚  ChromaDB Vector  â”‚   â”‚
â”‚  â”‚              â”‚      â”‚  Store            â”‚   â”‚
â”‚  â”‚  - Embeddingsâ”‚      â”‚  (chroma_db/)     â”‚   â”‚
â”‚  â”‚  - Search    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚  - Chunks    â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â–²                                       â”‚
â”‚         â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚  Libros PDF â”‚                                â”‚
â”‚  â”‚  (knowledge_â”‚                                â”‚
â”‚  â”‚   base/)    â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Guarda en PostgreSQL
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BD PostgreSQL  â”‚
â”‚  - Model        â”‚
â”‚  - LLMRequest   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n {#instalaciÃ³n}

### 1. Instalar dependencias

```powershell
cd "C:\Users\jhonm\Documents\TwinSec V2\Backend"
pip install -r requirements.txt
```

**Dependencias principales:**
- `openai==1.3.0` - Cliente OpenAI API
- `anthropic==0.7.0` - Cliente Anthropic (Claude)
- `langchain==0.1.0` - Framework RAG
- `chromadb==0.4.18` - Vector store local
- `sentence-transformers==2.2.2` - Embeddings
- `pypdf==3.17.4` - Procesar PDFs


## ğŸ“– Subir y Indexar Libros {#libros}

### 1. Subir PDFs a la carpeta

```powershell
# Estructura:
Backend/
â””â”€â”€ knowledge_base/
    â”œâ”€â”€ books/           # PDFs de libros aquÃ­
    â”‚   â”œâ”€â”€ ogata_control_systems.pdf
    â”‚   â”œâ”€â”€ franklin_feedback_control.pdf
    â”‚   â””â”€â”€ scada_security.pdf
    â””â”€â”€ docs/            # Documentos auxiliares
        â””â”€â”€ transfer_functions_cheatsheet.pdf
```

**Libros recomendados:**
1. **Control Systems:**
   - Ogata - "Modern Control Engineering"
   - Franklin - "Feedback Control of Dynamic Systems"
   - Nise - "Control Systems Engineering"

2. **Differential Equations:**
   - Boyce & DiPrima - "Elementary Differential Equations"
   - Zill - "A First Course in Differential Equations"

3. **SCADA/OT Security:**
   - Stouffer - "Guide to Industrial Control Systems (ICS) Security"
   - Weiss - "Protecting Industrial Control Systems from Electronic Threats"

### 2. Indexar los libros

```powershell
python Backend/scripts/index_knowledge_base.py
```

**Salida esperada:**
```
ğŸ” Iniciando indexaciÃ³n de knowledge base...
ğŸ“š Encontrados 3 documentos en knowledge_base/
ğŸ“„ Procesando: ogata_control_systems.pdf
ğŸ“„ Procesando: franklin_feedback_control.pdf
ğŸ“„ Procesando: scada_security.pdf
âœ… IndexaciÃ³n completa: 3 documentos, 1,247 chunks
ğŸ’¾ Vectorstore guardado en: Backend/chroma_db/
```

**Â¿QuÃ© hace el script?**
1. Lee todos los PDFs de `knowledge_base/`
2. Los divide en chunks de 1000 caracteres (200 overlap)
3. Genera embeddings con `all-MiniLM-L6-v2`
4. Los guarda en ChromaDB (local)

### 3. Verificar indexaciÃ³n

```python
from app.services.rag_service import get_rag_service

rag = get_rag_service()

# Buscar fragmentos relevantes
results = rag.retrieve_context(
    query="PID controller tuning",
    top_k=3
)

for i, result in enumerate(results):
    print(f"\n--- Resultado {i+1} ---")
    print(f"Score: {result['score']:.3f}")
    print(f"Source: {result['metadata']['source']}")
    print(f"Content: {result['content'][:200]}...")
```

---

## ğŸš€ Uso de la API {#uso-api}

### 1. Iniciar el servidor FastAPI

```powershell
cd "C:\Users\jhonm\Documents\TwinSec V2\Backend\api"
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Acceso:**
- API: `http://localhost:8000`
- DocumentaciÃ³n interactiva: `http://localhost:8000/docs`

### 2. Generar un modelo con LLM + RAG

**Request:**
```http
POST http://localhost:8000/api/v1/models/generate
Content-Type: application/json
Authorization: Bearer YOUR_JWT_TOKEN

{
  "prompt": "A water tank system with:\n- Tank capacity: 10 mÂ³\n- Inlet valve with max flow 0.5 mÂ³/s\n- Outlet valve with max flow 0.3 mÂ³/s\n- Level sensor (0-10 meters)\n- PID controller to maintain level at 5 meters",
  "model_type": "tank",
  "use_rag": true
}
```

**Response:**
```json
{
  "success": true,
  "model": {
    "metadata": {
      "name": "Water Tank Level Control",
      "description": "Single tank with PID controller",
      "model_type": "tank",
      "version": "1.0"
    },
    "solver": {
      "method": "rk4",
      "timestep": 0.1,
      "duration": 300
    },
    "components": [
      {
        "id": "tank1",
        "type": "tank",
        "parameters": {
          "area": 2.5,
          "max_level": 10.0,
          "initial_level": 3.0
        }
      },
      {
        "id": "inlet_valve",
        "type": "valve",
        "parameters": {
          "max_flow": 0.5,
          "cv": 1.2
        }
      }
    ],
    "connections": [
      {
        "from": "inlet_valve.flow_out",
        "to": "tank1.flow_in"
      }
    ],
    "signals": [
      {"id": "tank1.level", "type": "float", "unit": "m"},
      {"id": "inlet_valve.position", "type": "float", "unit": "%"}
    ],
    "hmi": [
      {
        "type": "gauge",
        "signal": "tank1.level",
        "min": 0,
        "max": 10,
        "label": "Tank Level (m)"
      }
    ]
  },
  "model_id": 123,
  "llm_request_id": 456,
  "metadata": {
    "cost_usd": 0.001234,
    "latency_ms": 2345,
    "total_tokens": 1750,
    "rag_context_used": true,
    "rag_fragments_count": 5
  }
}
```

### 3. Listar modelos del usuario

```http
GET http://localhost:8000/api/v1/models/?model_type=tank&skip=0&limit=10
Authorization: Bearer YOUR_JWT_TOKEN
```

### 4. Obtener modelo especÃ­fico

```http
GET http://localhost:8000/api/v1/models/123
Authorization: Bearer YOUR_JWT_TOKEN
```

### 5. Eliminar modelo

```http
DELETE http://localhost:8000/api/v1/models/123
Authorization: Bearer YOUR_JWT_TOKEN
```

---

## ğŸ’¡ Ejemplos de Prompts {#ejemplos}

### Ejemplo 1: Sistema de tanques interconectados

```json
{
  "prompt": "Two interconnected water tanks:\n- Tank 1: 15 mÂ³ capacity, receives input flow\n- Tank 2: 10 mÂ³ capacity, receives overflow from Tank 1\n- Pipe connecting tanks: diameter 0.1m\n- Each tank has outlet valve\n- Level sensors on both tanks\n- Implement cascading control",
  "model_type": "tank",
  "use_rag": true
}
```

### Ejemplo 2: Sistema HVAC

```json
{
  "prompt": "HVAC system for a 200 mÂ² room:\n- Heating coil (50 kW)\n- Cooling coil (40 kW)\n- Air handler with variable speed fan\n- Temperature sensor\n- Humidity sensor\n- PI controller for temperature (setpoint: 22Â°C)\n- On/Off control for humidity",
  "model_type": "hvac",
  "use_rag": true
}
```

### Ejemplo 3: Microgrid

```json
{
  "prompt": "Simple microgrid:\n- Solar panel: 50 kW peak\n- Battery: 100 kWh, 50 kW max charge/discharge\n- Load: variable 20-40 kW\n- Grid connection: bidirectional\n- Energy management system with state machine",
  "model_type": "microgrid",
  "use_rag": true
}
```

### Ejemplo 4: Proceso quÃ­mico (reactor)

```json
{
  "prompt": "Continuous stirred tank reactor (CSTR):\n- Volume: 2 mÂ³\n- Exothermic reaction: A â†’ B\n- Cooling jacket with circulating water\n- Temperature control (target: 60Â°C)\n- Concentration sensor\n- Feed flow: 0.1 mÂ³/min",
  "model_type": "chemical_reactor",
  "use_rag": true
}
```

---

## ğŸ’° Monitoreo de Costos {#costos}

### 1. Tarifas de LLMs (Noviembre 2025)

| Proveedor | Modelo | Input ($/1M tokens) | Output ($/1M tokens) | Uso recomendado |
|-----------|--------|---------------------|----------------------|-----------------|
| OpenAI | gpt-4o-mini | $0.150 | $0.600 | âœ… **ProducciÃ³n** (recomendado) |
| OpenAI | gpt-4-turbo | $10.00 | $30.00 | Balance calidad/costo |
| OpenAI | gpt-4 | $30.00 | $60.00 | MÃ¡xima calidad |
| Anthropic | claude-haiku | $0.25 | $1.25 | MÃ¡s rÃ¡pido |
| Anthropic | claude-sonnet-4.5 | $3.00 | $15.00 | Balance |
| Anthropic | claude-opus | $15.00 | $75.00 | MÃ¡xima calidad |

### 2. EstimaciÃ³n de costos por modelo

**Modelo tÃ­pico (gpt-4o-mini):**
- Prompt: ~250 tokens (system + user + RAG context)
- Respuesta: ~1500 tokens (JSON completo)
- **Costo total: ~$0.0011 USD por modelo**

**Con RAG (5 fragmentos):**
- Prompt: ~800 tokens (incluye contexto de libros)
- Respuesta: ~1800 tokens
- **Costo total: ~$0.0014 USD por modelo**

**Escenario mensual (100 modelos/mes):**
- 100 modelos Ã— $0.0014 = **$0.14 USD/mes** ğŸ‰

### 3. Ver costos en la BD

```sql
-- Costo total por usuario
SELECT 
    u.username,
    COUNT(llm.id) as total_requests,
    SUM(llm.cost_usd) as total_cost_usd,
    AVG(llm.latency_ms) as avg_latency_ms
FROM llm_requests llm
JOIN users u ON llm.user_id = u.id
WHERE llm.success = true
GROUP BY u.id, u.username
ORDER BY total_cost_usd DESC;

-- Costo por modelo LLM
SELECT 
    provider,
    model_name,
    COUNT(*) as requests,
    SUM(cost_usd) as total_cost,
    AVG(cost_usd) as avg_cost,
    AVG(latency_ms) as avg_latency
FROM llm_requests
WHERE success = true
GROUP BY provider, model_name;

-- Requests fallidos (para debugging)
SELECT 
    id,
    provider,
    model_name,
    error_message,
    created_at
FROM llm_requests
WHERE success = false
ORDER BY created_at DESC
LIMIT 10;
```

### 4. OptimizaciÃ³n de costos

**âœ… Recomendaciones:**

1. **Usar gpt-4o-mini** (5x mÃ¡s barato que GPT-4)
2. **Activar RAG** para mejor calidad sin aumentar mucho el costo
3. **Limitar `max_tokens`** a 4096 (suficiente para un modelo)
4. **Temperatura 0.0** (determinÃ­stico, evita re-generaciones)
5. **Cachear modelos comunes** (evitar regenerar el mismo modelo)

**ComparaciÃ³n con fine-tuning:**
| Enfoque | Costo inicial | Costo por inferencia | Tiempo setup | Calidad |
|---------|---------------|----------------------|--------------|---------|
| RAG + gpt-4o-mini | $0 | $0.0014 | 1 hora | 90% |
| Fine-tuning GPT-3.5 | $24 | $0.012 | 3 dÃ­as | 95% |
| Fine-tuning GPT-4 | $300+ | $0.06 | 1 semana | 98% |

**ConclusiÃ³n:** RAG + gpt-4o-mini es **17x mÃ¡s barato** que fine-tuning y alcanza 90% de calidad.

---

## ğŸ” Debugging

### Ver logs del LLM service

```python
import logging

logging.basicConfig(level=logging.DEBUG)

from app.services.llm_service import get_llm_service

llm = get_llm_service()
# VerÃ¡s logs detallados de cada paso
```

### Testear RAG manualmente

```python
from app.services.rag_service import get_rag_service

rag = get_rag_service()

# Buscar contexto
results = rag.retrieve_context("PID controller", top_k=3)

print(f"Encontrados {len(results)} fragmentos relevantes:")
for r in results:
    print(f"  - Score: {r['score']:.2f}, Source: {r['metadata']['source']}")
```

### Forzar re-indexaciÃ³n

```powershell
# Borrar vectorstore
Remove-Item -Recurse -Force Backend/chroma_db

# Re-indexar
python Backend/scripts/index_knowledge_base.py
```

---

## ğŸ“Š PrÃ³ximos Pasos

Ahora que tienes el sistema LLM + RAG funcionando:

1. âœ… **Objetivo 1 completado:** RAG para generaciÃ³n de modelos
2. â­ï¸ **Objetivo 2:** Implementar inyecciÃ³n de ataques (FDI, DoS, Replay)
3. â­ï¸ **Objetivo 3:** Evaluar frontend y migraciÃ³n
4. â­ï¸ **Objetivo 4:** IDS con Autoencoder + SHAP

**Â¿Continuar con Objetivo 2 (ataques)?**
